# Copy to scripts/.env and fill in your settings
# cp scripts/.env.example scripts/.env

# ── Authentication ──────────────────────────────────
# "google" = Google SSO (opens browser for manual sign-in)
# "email"  = email + password (auto-fills login form)
LOGIN_METHOD=google

# Email/password — only needed if LOGIN_METHOD=email
# OD_EMAIL=your@email.com
# OD_PASSWORD=yourpassword

# ── Chrome Profile ──────────────────────────────────
# Path to a Chrome user data directory for the scraper.
# A dedicated profile avoids conflicts with your main browser.
# Leave blank to auto-create .chrome-scraper-profile/ in the repo root.
#
# To use an existing Chrome profile instead:
#   macOS:  ~/Library/Application Support/Google/Chrome
#   Linux:  ~/.config/google-chrome
#   Windows: %LOCALAPPDATA%\Google\Chrome\User Data
#
# CHROME_PROFILE_PATH=

# ── Data Settings ───────────────────────────────────
# Only extract strikes above this level (filters out far-OTM noise)
STRIKE_FLOOR=5500

# Which table column to read (1 = second column, the metric value)
COLUMN_INDEX=1

# Where to save extracted JSON (relative to repo root)
OUTPUT_PATH=./latest_data.json
